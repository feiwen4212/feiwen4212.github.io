---
layout: post
title:  "强化学习关键词"
date:   2018-10-01 17:07:10 +0530
categories: notes
---

## **强化学习基础概念**
强化学习是agent和enviroment相互交互反馈的过程中的学习策略。于强化学习相似的两类学习方法监督学习和非监督学习，前者有专家事先打标目标位最优化标签匹配的目标函数，后者是在没有打标的数据中挖掘隐藏结构的学习策略。从这个角度看强化学习似乎更加类似非监督学习，因为强化学习是也是一类没有预先标签的学习方法，但强化学习的学习策略是优化目标函数儿非挖掘数据隐藏结构，从这来看强化学习于监督学习存在很多类同。

![image](https://lilianweng.github.io/lil-log/assets/images/agent_environment_MDP.png)
Fig 1：MDP中中enviroment和aget的交互

### **关键词**
1. **enviroment** 于 **agent**交互关系参考上图，这里称之为**环境**于**智能体**。智能体的目标是最大化与环境交互中的**累计奖励**（maximize **cumulative rewards**）。
2. enviroment与**model**是区别的概念，环境是智能体交互的主体，而环境对智能体特定动作做出反馈称之为**模型**，是对环境的描述，在这里模型是既可以是可知的也可以是不可知的。
    * **model-based** RL 基于模型的强化学习拥有对环境的完全了解，模型训练依赖对环境模型的了解例如DP算法。
    * **model-free** RL 对环境不完全了解，在学习过程中不依赖环境模型。
3. **reward**（奖励），**state**（状态），**action**（动作）如Fig1中是描述智能体和环境在交互过程中传递的信息，
4. **policy**策略是智能体动作决策的映射函数，两类常见的策略 
    * **Deterministic policy**: $\pi(s) = a$
    * **Stocastic policy**: $\pi(s,a) = P(A_t= a,| S_t=s)$
5. **Transition**是交互中的一个循环$s\rightarrow a \rightarrow s' \rightarrow r$，可用元组表示为(s, a, s’, r)。其中$s \rightarrow a$描述智能体的动作策略，可以用**policy** $\pi$ 描述智能体的决策的概率。$s\rightarrow a \rightarrow s' \rightarrow r$描述环境中的状态转移，可用状态转移概率$P_{s,a}^{s'} = P(S'=s'|S=s,A=a)$表示。
### **Value**
1. 智能体学习的目标是最大化累计奖励，也可以称之为最大化**return**(回报)，可以用**sum of rewards**（累计奖励）表示$G=\sum_{t=0}^{\inf}r_0$。在实际训练中回报的计算往往会使用**sum of discounted rewards**（累计折扣奖励）表示$G=\sum_{t=0}^{\inf}\gamma^tr_0$，其中**discount**折扣因子取值在[0，1]。这么做有以下几点好处
   * 长期的奖励存在不确定性，回报的计算中更加看中近期的收益。
   * 折扣系数可以避免无限长的transition中产生无穷大的回报，这也方便了回报的计算。
2. **state—value**是在特定状态时的未来期望回报,$V(s)=E[G_t| S_t=s]$，类似的有Q值$Q(s,a)=E[G_t| S_t=s， A_t=a]$。在策略$\pi$下，t时的V和Q存在关系$V_\pi(s)=\sum_{a\in A}\pi(a|s)Q(s,a)$。
### **MDP卡尔科夫决策过程**
强化学习是一个典型的马尔可夫决策过程问题，在这类问题中，根据makov性质我们把状态转移概率简化为$P(S_{t+1}|S_0,S_1,...,S_t) = P(S_{t+1}|S_t)$，即下一个状态只依赖当前状态而非历史状态。
### **Bellman Equations贝尔曼方程**
贝尔曼公式是RL中用来描述值迭代的最重要的形式，在训练拟合策略中也是最常用到的。
$$
\begin{aligned}
V(s) &= E[G_t|S_t=s] \\
&=E[r+t + \gamma r_{t+1} + \gamma^2 r_{t+2} + ...|s_t=s] \\
&=E[r+t + \gamma (r_{t+1} + \gamma r_{t+2} + ...)|s_t=s] \\
&=E[r_t + \gamma G_t|S_t=s] \\
&=E[r_t + \gamma V(s_{t+1})|S_t=s]
\end{aligned}
$$

同时也有Q值形式

$$
\begin{aligned}
Q(s,a) &= E[r_{t+1} + \gamma V(s_{t+1}) |S_t=s, A_t=a] \\
&= E[r_{t+1} + \gamma E_{a' \sim \pi}Q(s_{t+1},a') |S_t=s, A_t=a]
\end{aligned}
$$
#### **贝尔曼期望方程**

$$
V_\pi(s) = \sum_{a \in A} \pi(a
|s) Q_\pi(s, a) \\
Q_\pi(s,a) = r(s, a) + \gamma \sum_{s' \in S}P(S_{t+1} = s'|S_t = s, A_t = a)V_\pi(s') \\
V_\pi (s) = \sum_{a \in A} \pi(a|s) [r(s, a) + \gamma \sum_{s' \in S}P(S_{t+1} = s'|S_t = s, A_t = a)V_\pi(s') ] \\
Q_\pi(s,a) = r(s, a) + \gamma \sum_{s' \in S}P(S_{t+1} = s'|S_t = s, A_t = a) \sum_{a' \in A} \pi(a'
|s') Q_\pi(s', a') \\
$$

### **贝尔曼最优方程**
$$
V^*(s) = \max_{a\in A} Q^*(s,a) \\
Q^*(s,a) = r(s, a) + \gamma \sum_{s' \in S}P(S_{t+1} = s'|S_t = s, A_t = a)V^*(s') \\
V^* (s) = \max_{a \in A} [r(s, a) + \gamma \sum_{s' \in S}P(S_{t+1} = s'|S_t = s, A_t = a)V^*(s') ] \\
Q^*(s,a) = r(s, a) + \gamma \sum_{s' \in S}P(S_{t+1} = s'|S_t = s, A_t = a) \max_{a' \in a} Q^*s', a') \\
$$


## 参考
[1] Sutton, R, and Barto, A. Reinforcement Learning:An Introduction. MIT Press, 1998.

[2] https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#policy